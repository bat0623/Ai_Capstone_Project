{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7676f083",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shado\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§© 1. ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ ì •ì˜\n",
    "import os\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "def build_dataset_from_folders(base_path: str = r\"C:\\github\\Ai_Capstone_Project\\data\") -> Dataset:\n",
    "    examples = []\n",
    "    char_attrs_by_story = {}\n",
    "\n",
    "    # ìºë¦­í„° ì •ì˜ ë¡œë“œ\n",
    "    char_defs_path = os.path.join(base_path, \"characters\", \"Characters__definitions.json\")\n",
    "    if os.path.isfile(char_defs_path):\n",
    "        with open(char_defs_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            char_defs_data = json.load(f)\n",
    "        for story_name, char_list in char_defs_data.items():\n",
    "            story_map = {}\n",
    "            for char in char_list:\n",
    "                name = char.get(\"ì´ë¦„\") or char.get(\"name\")\n",
    "                if not name:\n",
    "                    continue\n",
    "                occupation = char.get(\"ì§ì—…êµ°_ë¶„ì•¼\")\n",
    "                roles = char.get(\"ì§ì—…êµ°_ì—­í• \")\n",
    "                social_status = char.get(\"ê³µí†µê³„ì¸µ\")\n",
    "                era = char.get(\"ì‹œëŒ€\")\n",
    "                age = char.get(\"ë‚˜ì´\")\n",
    "                desc_parts = []\n",
    "                if era: desc_parts.append(f\"{era} ì‹œëŒ€\")\n",
    "                if social_status: desc_parts.append(f\"{social_status} ì‹ ë¶„\")\n",
    "                if occupation: desc_parts.append(occupation)\n",
    "                if roles:\n",
    "                    role_text = \" ë° \".join(roles) if isinstance(roles, list) else str(roles)\n",
    "                    desc_parts.append(f\"{role_text} ì—­í• \")\n",
    "                if age: desc_parts.append(age)\n",
    "                char_description = f\"{name}ì€(ëŠ”) {' '.join(desc_parts)}ì´ë‹¤.\" if desc_parts else name\n",
    "                story_map[name] = char_description\n",
    "            char_attrs_by_story[story_name] = story_map\n",
    "\n",
    "    # í´ë” ìˆœíšŒ\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        if os.path.basename(root).lower() in [\"definitions\", \"characters\"]:\n",
    "            continue\n",
    "        for fname in files:\n",
    "            if not fname.endswith(\".json\") or \"definitions\" in fname.lower():\n",
    "                continue\n",
    "            file_path = os.path.join(root, fname)\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            story_key = os.path.splitext(fname)[0]\n",
    "            if story_key not in char_attrs_by_story and \"_\" in story_key:\n",
    "                alt_key = story_key.replace(\"_\", \" \")\n",
    "                if alt_key in char_attrs_by_story:\n",
    "                    story_key = alt_key\n",
    "\n",
    "            # case: dict {ìºë¦­í„°ì´ë¦„: [ëŒ€ì‚¬]}\n",
    "            if isinstance(data, dict):\n",
    "                for char_name, utterances in data.items():\n",
    "                    if not isinstance(utterances, list):\n",
    "                        continue\n",
    "                    char_desc = char_attrs_by_story.get(story_key, {}).get(char_name, char_name)\n",
    "                    for entry in utterances:\n",
    "                        if not isinstance(entry, dict) or \"text\" not in entry:\n",
    "                            continue\n",
    "                        text = entry[\"text\"]\n",
    "                        traits = entry.get(\"traits\") or entry.get(\"ì„±ê²©\") or []\n",
    "                        emotion = entry.get(\"emotion\") or entry.get(\"ê°ì •\") or []\n",
    "                        traits = [traits] if isinstance(traits, str) else traits\n",
    "                        emotion = [emotion] if isinstance(emotion, str) else emotion\n",
    "                        prompt = f\"ìºë¦­í„° ì„¤ëª…: {char_desc}\\nê°ì •: {', '.join(emotion)}\\nì„±ê²©: {', '.join(traits)}\"\n",
    "                        examples.append({\"prompt\": prompt, \"response\": text})\n",
    "\n",
    "            # case: list [{character, text, emotion, ...}]\n",
    "            elif isinstance(data, list):\n",
    "                for entry in data:\n",
    "                    if not isinstance(entry, dict): continue\n",
    "                    char_name = entry.get(\"character\") or entry.get(\"speaker\") or entry.get(\"name\")\n",
    "                    text = entry.get(\"text\")\n",
    "                    if not text or not char_name: continue\n",
    "                    char_desc = char_attrs_by_story.get(story_key, {}).get(char_name, char_name)\n",
    "                    traits = entry.get(\"traits\") or entry.get(\"ì„±ê²©\") or []\n",
    "                    emotion = entry.get(\"emotion\") or entry.get(\"ê°ì •\") or []\n",
    "                    traits = [traits] if isinstance(traits, str) else traits\n",
    "                    emotion = [emotion] if isinstance(emotion, str) else emotion\n",
    "                    prompt = f\"ìºë¦­í„° ì„¤ëª…: {char_desc}\\nê°ì •: {', '.join(emotion)}\\nì„±ê²©: {', '.join(traits)}\"\n",
    "                    examples.append({\"prompt\": prompt, \"response\": text})\n",
    "\n",
    "    return Dataset.from_list(examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ffbff8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shado\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:07<00:00,  1.01it/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3482/3482 [00:01<00:00, 3348.04 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” ì‹œì‘ Epoch 1...\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 83\u001b[0m\n\u001b[0;32m     80\u001b[0m epoch_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mğŸ” ì‹œì‘ Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 83\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch_count\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite_output_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ë°°ì¹˜ ì‚¬ì´ì¦ˆ ê°ì†Œ\u001b[39;49;00m\n\u001b[0;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ë‹¨ê³„ ê°ì†Œ\u001b[39;49;00m\n\u001b[0;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mno\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mno\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     95\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     98\u001b[0m     model\u001b[38;5;241m=\u001b[39mbase_model,\n\u001b[0;32m     99\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_dataset,\n\u001b[0;32m    100\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m    101\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator\n\u001b[0;32m    102\u001b[0m )\n\u001b[0;32m    104\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "\u001b[1;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from build_dataset_from_folders import build_dataset_from_folders\n",
    "\n",
    "# âœ… Windows ì „ìš©: CUDA DLL ê²½ë¡œ ë“±ë¡\n",
    "if platform.system() == \"Windows\":\n",
    "    dll_path = r\"C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8\\bin\"\n",
    "    if os.path.exists(dll_path):\n",
    "        os.add_dll_directory(dll_path)\n",
    "\n",
    "# âœ… ê²½ë¡œ ì„¤ì •\n",
    "model_path = r\"./polyglot-ko-3.8B\"\n",
    "output_dir = \"./finetuned-polyglot\"\n",
    "data_path = r\"C:\\github\\Ai_Capstone_Project\\data\"\n",
    "\n",
    "# âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id or 0\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# âœ… ëª¨ë¸ ë¡œë“œ (float16), ì‹¤íŒ¨ ì‹œ float32 fallback\n",
    "try:\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        load_in_8bit=False  # í•™ìŠµì´ë¯€ë¡œ ëª…ì‹œì ìœ¼ë¡œ False\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\"âŒ float16 ë¡œë”© ì‹¤íŒ¨. float32 fallback:\", e)\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        torch_dtype=torch.float32,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "\n",
    "# âœ… ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "dataset = build_dataset_from_folders(data_path)\n",
    "\n",
    "def tokenize_example(example):\n",
    "    prompt_ids = tokenizer(example[\"prompt\"], add_special_tokens=False)[\"input_ids\"]\n",
    "    response_ids = tokenizer(example[\"response\"], add_special_tokens=False)[\"input_ids\"]\n",
    "    input_ids = prompt_ids + response_ids\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "    labels = [-100] * len(prompt_ids) + response_ids\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_example, remove_columns=[\"prompt\", \"response\"])\n",
    "\n",
    "# âœ… ë™ì  íŒ¨ë”©\n",
    "def data_collator(batch):\n",
    "    max_len = max(len(x[\"input_ids\"]) for x in batch)\n",
    "    pad_id = tokenizer.pad_token_id\n",
    "    input_ids, attention_masks, labels = [], [], []\n",
    "    for x in batch:\n",
    "        pad_len = max_len - len(x[\"input_ids\"])\n",
    "        input_ids.append(x[\"input_ids\"] + [pad_id] * pad_len)\n",
    "        attention_masks.append(x[\"attention_mask\"] + [0] * pad_len)\n",
    "        labels.append(x[\"labels\"] + [-100] * pad_len)\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor(attention_masks, dtype=torch.long),\n",
    "        \"labels\": torch.tensor(labels, dtype=torch.long)\n",
    "    }\n",
    "\n",
    "# âœ… í•™ìŠµ ë£¨í”„ (ëª©í‘œ Loss ë§Œì¡± ì „ê¹Œì§€ ë°˜ë³µ)\n",
    "current_loss = float(\"inf\")\n",
    "target_loss = 1.0\n",
    "epoch_count = 0\n",
    "max_epochs = 10\n",
    "\n",
    "while current_loss > target_loss and epoch_count < max_epochs:\n",
    "    epoch_count += 1\n",
    "    print(f\"\\nğŸ” ì‹œì‘ Epoch {epoch_count}...\\n\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=os.path.join(output_dir, f\"epoch{epoch_count}\"),\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=1,  # ë°°ì¹˜ ì‚¬ì´ì¦ˆ ê°ì†Œ\n",
    "        gradient_accumulation_steps=4,  # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ë‹¨ê³„ ê°ì†Œ\n",
    "        evaluation_strategy=\"no\",\n",
    "        save_strategy=\"no\",\n",
    "        learning_rate=2e-4,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        logging_steps=20,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=base_model,\n",
    "        train_dataset=tokenized_dataset,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_output = trainer.evaluate()\n",
    "    current_loss = eval_output.get(\"eval_loss\", float(\"inf\"))\n",
    "    print(f\"ğŸ“‰ í˜„ì¬ eval_loss: {current_loss:.4f}\")\n",
    "\n",
    "# âœ… í•™ìŠµ ê²°ê³¼ ì €ì¥\n",
    "base_model.save_pretrained(os.path.join(output_dir, \"final\"))\n",
    "print(f\"âœ… íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {output_dir}/final\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
