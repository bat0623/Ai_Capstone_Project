import os
import torch
import torch.nn as nn
import torch.optim as optim
from load_dataset import load_data
from cnn_model import CNNModel

# âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
data_dir = "data_set"
seq_len = 20
batch_size = 32
epochs = 5
learning_rate = 0.001
model_path = "trained_model.pth"

# âœ… ë°ì´í„°ì…‹ ë¡œë“œ
print("ğŸ“‚ `data_set/` í´ë” ë‚´ ëª¨ë“  JSON íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
train_loader, dataset = load_data(data_dir, seq_len=seq_len, batch_size=batch_size)
vocab_size = dataset.vocab_size
print(f"ğŸ“ ì–´íœ˜ ì‚¬ì „ í¬ê¸°: {vocab_size} ê°œ ë‹¨ì–´")

# âœ… ëª¨ë¸ ì´ˆê¸°í™”
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = CNNModel(vocab_size=vocab_size).to(device)

# âœ… ê¸°ì¡´ ëª¨ë¸ì´ ì¡´ì¬í•˜ë©´ ë¶ˆëŸ¬ì™€ì„œ ì—°ì¥ í•™ìŠµ ì§„í–‰
if os.path.exists(model_path):
    model.load_state_dict(torch.load(model_path, map_location=device))
    print("ğŸ”„ ê¸°ì¡´ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ ì¶”ê°€ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.")
else:
    print("ğŸ†• ìƒˆë¡œìš´ ëª¨ë¸ì„ ìƒì„±í•˜ì—¬ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")

# âœ… ì†ì‹¤ í•¨ìˆ˜ ë° ì˜µí‹°ë§ˆì´ì € ì •ì˜
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# âœ… í•™ìŠµ ë£¨í”„
model.train()
for epoch in range(1, epochs + 1):
    total_loss = 0.0
    for batch_idx, (inputs, targets) in enumerate(train_loader):
        inputs, targets = inputs.to(device), targets.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()
    
    avg_loss = total_loss / len(train_loader)
    print(f"ğŸ“¢ Epoch {epoch}/{epochs} - í‰ê·  ì†ì‹¤: {avg_loss:.4f}")

# âœ… í•™ìŠµ ì™„ë£Œ í›„ ëª¨ë¸ ì €ì¥
torch.save(model.state_dict(), model_path)
print(f"âœ… í•™ìŠµëœ ëª¨ë¸ì´ '{model_path}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
