{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (2295692657.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install konlpy==0.6.0\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "#load_dataset.py\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from eunjeon import Mecab\n",
    "from glob import glob\n",
    "\n",
    "# Mecab 형태소 분석기 초기화\n",
    "mecab = Mecab()\n",
    "\n",
    "# 데이터셋 폴더 경로\n",
    "DATASET_DIR = \"./data_set\"\n",
    "\n",
    "# JSON 파일 로드\n",
    "def load_json_files(dataset_dir):\n",
    "    dataset = []\n",
    "    json_files = glob(os.path.join(dataset_dir, \"*.json\"))\n",
    "\n",
    "    for json_file in json_files:\n",
    "        with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            dataset.extend(data[\"dataset\"])\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# 어휘 사전 만들기 (word2idx)\n",
    "def build_vocab(dataset):\n",
    "    vocab = set()\n",
    "    for item in dataset:\n",
    "        vocab.update(item[\"tokens\"])\n",
    "\n",
    "    word2idx = {word: idx + 1 for idx, word in enumerate(sorted(vocab))}\n",
    "    word2idx[\"<PAD>\"] = 0  # 패딩 추가\n",
    "    return word2idx\n",
    "\n",
    "# 데이터셋 정의\n",
    "class KoreanTextDataset(Dataset):\n",
    "    def __init__(self, dataset, word2idx, max_len=20):\n",
    "        self.dataset = dataset\n",
    "        self.word2idx = word2idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        text = item[\"text\"]\n",
    "        tokens = item[\"tokens\"]\n",
    "\n",
    "        # 토큰을 인덱스로 변환\n",
    "        indexed_tokens = [self.word2idx.get(tok, 0) for tok in tokens]\n",
    "\n",
    "        # 패딩 추가\n",
    "        if len(indexed_tokens) < self.max_len:\n",
    "            indexed_tokens += [0] * (self.max_len - len(indexed_tokens))\n",
    "        else:\n",
    "            indexed_tokens = indexed_tokens[:self.max_len]\n",
    "\n",
    "        return torch.tensor(indexed_tokens), text\n",
    "\n",
    "# 전체 데이터 로드\n",
    "dataset = load_json_files(DATASET_DIR)\n",
    "word2idx = build_vocab(dataset)\n",
    "train_dataset = KoreanTextDataset(dataset, word2idx)\n",
    "\n",
    "# 데이터 로더 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 데이터 확인\n",
    "for batch in train_loader:\n",
    "    inputs, texts = batch\n",
    "    print(f\"입력 텐서: {inputs.shape}\")  # (32, 20)\n",
    "    print(f\"샘플 텍스트: {texts[:2]}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn_model.py\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, num_filters=100, filter_sizes=[2,3,4]):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filters, (fs, embed_dim)) for fs in filter_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(num_filters * len(filter_sizes), vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).unsqueeze(1)\n",
    "        conv_x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "        pooled_x = [F.max_pool1d(c, c.size(2)).squeeze(2) for c in conv_x]\n",
    "        out = torch.cat(pooled_x, dim=1)\n",
    "        return self.fc(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.py\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from load_dataset import train_dataset, word2idx  # 데이터셋 불러오기\n",
    "from cnn_model import TextCNN  # CNN 모델 불러오기\n",
    "\n",
    "# GPU 또는 CPU 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 모델 경로 설정\n",
    "MODEL_PATH = \"trained_model.pth\"\n",
    "\n",
    "# 기존 모델이 있다면 불러오기 (Fine-tuning 지원)\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(\"🔄 기존 모델을 불러와 추가 학습을 진행합니다...\")\n",
    "    model = TextCNN(len(word2idx)).to(device)\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "else:\n",
    "    print(\"🆕 새로운 모델을 생성하여 학습을 시작합니다...\")\n",
    "    model = TextCNN(len(word2idx)).to(device)\n",
    "\n",
    "# 옵티마이저 및 손실 함수 정의\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 데이터 로더 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 학습 루프\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        inputs, _ = batch\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        targets = inputs[:, 1:]  # Shifted target (예제)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# 학습 완료 후 모델 저장\n",
    "torch.save(model.state_dict(), MODEL_PATH)\n",
    "print(f\"✅ 학습 완료! 모델이 '{MODEL_PATH}' 파일로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_model.py\n",
    "import os\n",
    "import torch\n",
    "from eunjeon import Mecab\n",
    "from load_dataset import word2idx, idx2word  # 기존 데이터셋에서 불러옴\n",
    "from cnn_model import TextCNN  # CNN 모델 로드\n",
    "\n",
    "# 모델 경로 설정\n",
    "MODEL_PATH = \"trained_model.pth\"\n",
    "\n",
    "# GPU 또는 CPU 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 모델 불러오기\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(\"📥 저장된 모델을 불러옵니다...\")\n",
    "    model = TextCNN(len(word2idx)).to(device)\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    model.eval()\n",
    "else:\n",
    "    print(\"❌ 저장된 모델이 없습니다. 먼저 train.py를 실행하여 학습해주세요.\")\n",
    "    exit()\n",
    "\n",
    "# 형태소 분석기 초기화\n",
    "mecab = Mecab()\n",
    "\n",
    "def generate_text(model, word2idx, input_text, max_len=20):\n",
    "    tokens = mecab.morphs(input_text)\n",
    "    input_idx = [word2idx.get(tok, 0) for tok in tokens]\n",
    "\n",
    "    # 패딩 적용\n",
    "    if len(input_idx) < max_len:\n",
    "        input_idx += [0] * (max_len - len(input_idx))\n",
    "    else:\n",
    "        input_idx = input_idx[:max_len]\n",
    "\n",
    "    input_tensor = torch.tensor([input_idx]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "\n",
    "    predicted_idx = torch.argmax(output, dim=-1).cpu().numpy()\n",
    "    predicted_text = [idx2word.get(idx, \"<UNK>\") for idx in predicted_idx[0]]\n",
    "\n",
    "    return \" \".join(predicted_text)\n",
    "\n",
    "# 사용자 입력을 받아서 문장 생성\n",
    "while True:\n",
    "    user_input = input(\"\\n💬 문장을 입력하세요 (종료: exit): \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"🔚 종료합니다.\")\n",
    "        break\n",
    "\n",
    "    generated_sentence = generate_text(model, word2idx, user_input)\n",
    "    print(f\"📝 생성된 문장: {generated_sentence}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
