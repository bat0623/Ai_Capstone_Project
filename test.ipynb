{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (2295692657.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install konlpy==0.6.0\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "#load_dataset.py\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from eunjeon import Mecab\n",
    "from glob import glob\n",
    "\n",
    "# Mecab í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "mecab = Mecab()\n",
    "\n",
    "# ë°ì´í„°ì…‹ í´ë” ê²½ë¡œ\n",
    "DATASET_DIR = \"./data_set\"\n",
    "\n",
    "# JSON íŒŒì¼ ë¡œë“œ\n",
    "def load_json_files(dataset_dir):\n",
    "    dataset = []\n",
    "    json_files = glob(os.path.join(dataset_dir, \"*.json\"))\n",
    "\n",
    "    for json_file in json_files:\n",
    "        with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            dataset.extend(data[\"dataset\"])\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# ì–´íœ˜ ì‚¬ì „ ë§Œë“¤ê¸° (word2idx)\n",
    "def build_vocab(dataset):\n",
    "    vocab = set()\n",
    "    for item in dataset:\n",
    "        vocab.update(item[\"tokens\"])\n",
    "\n",
    "    word2idx = {word: idx + 1 for idx, word in enumerate(sorted(vocab))}\n",
    "    word2idx[\"<PAD>\"] = 0  # íŒ¨ë”© ì¶”ê°€\n",
    "    return word2idx\n",
    "\n",
    "# ë°ì´í„°ì…‹ ì •ì˜\n",
    "class KoreanTextDataset(Dataset):\n",
    "    def __init__(self, dataset, word2idx, max_len=20):\n",
    "        self.dataset = dataset\n",
    "        self.word2idx = word2idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        text = item[\"text\"]\n",
    "        tokens = item[\"tokens\"]\n",
    "\n",
    "        # í† í°ì„ ì¸ë±ìŠ¤ë¡œ ë³€í™˜\n",
    "        indexed_tokens = [self.word2idx.get(tok, 0) for tok in tokens]\n",
    "\n",
    "        # íŒ¨ë”© ì¶”ê°€\n",
    "        if len(indexed_tokens) < self.max_len:\n",
    "            indexed_tokens += [0] * (self.max_len - len(indexed_tokens))\n",
    "        else:\n",
    "            indexed_tokens = indexed_tokens[:self.max_len]\n",
    "\n",
    "        return torch.tensor(indexed_tokens), text\n",
    "\n",
    "# ì „ì²´ ë°ì´í„° ë¡œë“œ\n",
    "dataset = load_json_files(DATASET_DIR)\n",
    "word2idx = build_vocab(dataset)\n",
    "train_dataset = KoreanTextDataset(dataset, word2idx)\n",
    "\n",
    "# ë°ì´í„° ë¡œë” ì„¤ì •\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# ë°ì´í„° í™•ì¸\n",
    "for batch in train_loader:\n",
    "    inputs, texts = batch\n",
    "    print(f\"ì…ë ¥ í…ì„œ: {inputs.shape}\")  # (32, 20)\n",
    "    print(f\"ìƒ˜í”Œ í…ìŠ¤íŠ¸: {texts[:2]}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn_model.py\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, num_filters=100, filter_sizes=[2,3,4]):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filters, (fs, embed_dim)) for fs in filter_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(num_filters * len(filter_sizes), vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).unsqueeze(1)\n",
    "        conv_x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "        pooled_x = [F.max_pool1d(c, c.size(2)).squeeze(2) for c in conv_x]\n",
    "        out = torch.cat(pooled_x, dim=1)\n",
    "        return self.fc(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.py\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from load_dataset import train_dataset, word2idx  # ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "from cnn_model import TextCNN  # CNN ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "\n",
    "# GPU ë˜ëŠ” CPU ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ëª¨ë¸ ê²½ë¡œ ì„¤ì •\n",
    "MODEL_PATH = \"trained_model.pth\"\n",
    "\n",
    "# ê¸°ì¡´ ëª¨ë¸ì´ ìˆë‹¤ë©´ ë¶ˆëŸ¬ì˜¤ê¸° (Fine-tuning ì§€ì›)\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(\"ğŸ”„ ê¸°ì¡´ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ ì¶”ê°€ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤...\")\n",
    "    model = TextCNN(len(word2idx)).to(device)\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "else:\n",
    "    print(\"ğŸ†• ìƒˆë¡œìš´ ëª¨ë¸ì„ ìƒì„±í•˜ì—¬ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "    model = TextCNN(len(word2idx)).to(device)\n",
    "\n",
    "# ì˜µí‹°ë§ˆì´ì € ë° ì†ì‹¤ í•¨ìˆ˜ ì •ì˜\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ë°ì´í„° ë¡œë” ì„¤ì •\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# í•™ìŠµ ë£¨í”„\n",
    "EPOCHS = 5\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        inputs, _ = batch\n",
    "        inputs = inputs.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        targets = inputs[:, 1:]  # Shifted target (ì˜ˆì œ)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "# í•™ìŠµ ì™„ë£Œ í›„ ëª¨ë¸ ì €ì¥\n",
    "torch.save(model.state_dict(), MODEL_PATH)\n",
    "print(f\"âœ… í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ì´ '{MODEL_PATH}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_model.py\n",
    "import os\n",
    "import torch\n",
    "from eunjeon import Mecab\n",
    "from load_dataset import word2idx, idx2word  # ê¸°ì¡´ ë°ì´í„°ì…‹ì—ì„œ ë¶ˆëŸ¬ì˜´\n",
    "from cnn_model import TextCNN  # CNN ëª¨ë¸ ë¡œë“œ\n",
    "\n",
    "# ëª¨ë¸ ê²½ë¡œ ì„¤ì •\n",
    "MODEL_PATH = \"trained_model.pth\"\n",
    "\n",
    "# GPU ë˜ëŠ” CPU ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "if os.path.exists(MODEL_PATH):\n",
    "    print(\"ğŸ“¥ ì €ì¥ëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤...\")\n",
    "    model = TextCNN(len(word2idx)).to(device)\n",
    "    model.load_state_dict(torch.load(MODEL_PATH))\n",
    "    model.eval()\n",
    "else:\n",
    "    print(\"âŒ ì €ì¥ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € train.pyë¥¼ ì‹¤í–‰í•˜ì—¬ í•™ìŠµí•´ì£¼ì„¸ìš”.\")\n",
    "    exit()\n",
    "\n",
    "# í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™”\n",
    "mecab = Mecab()\n",
    "\n",
    "def generate_text(model, word2idx, input_text, max_len=20):\n",
    "    tokens = mecab.morphs(input_text)\n",
    "    input_idx = [word2idx.get(tok, 0) for tok in tokens]\n",
    "\n",
    "    # íŒ¨ë”© ì ìš©\n",
    "    if len(input_idx) < max_len:\n",
    "        input_idx += [0] * (max_len - len(input_idx))\n",
    "    else:\n",
    "        input_idx = input_idx[:max_len]\n",
    "\n",
    "    input_tensor = torch.tensor([input_idx]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "\n",
    "    predicted_idx = torch.argmax(output, dim=-1).cpu().numpy()\n",
    "    predicted_text = [idx2word.get(idx, \"<UNK>\") for idx in predicted_idx[0]]\n",
    "\n",
    "    return \" \".join(predicted_text)\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥ì„ ë°›ì•„ì„œ ë¬¸ì¥ ìƒì„±\n",
    "while True:\n",
    "    user_input = input(\"\\nğŸ’¬ ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: exit): \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"ğŸ”š ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "        break\n",
    "\n",
    "    generated_sentence = generate_text(model, word2idx, user_input)\n",
    "    print(f\"ğŸ“ ìƒì„±ëœ ë¬¸ì¥: {generated_sentence}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
