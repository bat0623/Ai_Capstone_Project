import os
import sys
import platform
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch

# âœ… Windows í™˜ê²½ì¼ ë•Œ CUDA DLL ê²½ë¡œ ì¶”ê°€
if platform.system() == "Windows":
    dll_path = "C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.8/bin"
    if os.path.exists(dll_path):
        os.add_dll_directory(dll_path)

# ëª¨ë¸ ê²½ë¡œ
model_path = "./polyglot-ko-3.8B"

# 8bit quantization êµ¬ì„±
bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
)

print("ğŸ”„ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)

try:
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        quantization_config=bnb_config,
        device_map="auto"
    )
except Exception as e:
    print("âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨:", e)
    print("âœ… float32 fallbackìœ¼ë¡œ ì¬ì‹œë„ ì¤‘...")
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float32,
        device_map="auto"
    )

# í”„ë¡¬í”„íŠ¸ ì˜ˆì‹œ
prompt = "Q: ì„¸ìƒì—ì„œ ê°€ì¥ ë§›ìˆëŠ” ìŒì‹ì€ ë­ì•¼?\nA:"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
inputs.pop("token_type_ids", None)

print("ğŸ§  í…ìŠ¤íŠ¸ ìƒì„± ì¤‘...")
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=40,
        do_sample=False,
        temperature=0.7,
        top_p=0.9,
        pad_token_id=tokenizer.eos_token_id
    )

result = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("\nğŸ“¢ ìƒì„±ëœ í…ìŠ¤íŠ¸:\n")
print(result)
