import json
import os
import torch
from torch.utils.data import Dataset, DataLoader

class TextDataset(Dataset):
    def __init__(self, data_dir, seq_len):
        """
        `data_set/` í´ë” ë‚´ ëª¨ë“  JSON íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ ë°ì´í„°ì…‹ì„ ìƒì„±.
        JSON íŒŒì¼ì˜ 'tokens' í•„ë“œë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ MeCab ì—†ì´ í•™ìŠµ ê°€ëŠ¥.
        """
        self.seq_len = seq_len
        self.sentences = []

        # `data_set/` í´ë” ë‚´ ëª¨ë“  JSON íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
        json_files = [f for f in os.listdir(data_dir) if f.endswith(".json")]

        if not json_files:
            raise FileNotFoundError(f"ğŸš¨ 'data_set/' í´ë”ì— JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤! {data_dir} í™•ì¸ í•„ìš”")

        for filename in json_files:
            file_path = os.path.join(data_dir, filename)
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                self.sentences.extend([entry["tokens"] for entry in data["dataset"]])

        # âœ… ì–´íœ˜ ì‚¬ì „ ìƒì„±
        unique_tokens = sorted(set(token for tokens in self.sentences for token in tokens))
        self.pad_token = "<PAD>"
        self.unk_token = "<UNK>"
        vocab = [self.pad_token, self.unk_token] + unique_tokens
        self.word2idx = {word: idx for idx, word in enumerate(vocab)}
        self.idx2word = {idx: word for word, idx in self.word2idx.items()}
        self.vocab_size = len(vocab)

        # âœ… ì…ë ¥ ì‹œí€€ìŠ¤ ë° íƒ€ê²Ÿ ìƒì„±
        self.X_data, self.y_data = [], []
        for tokens in self.sentences:
            if len(tokens) < seq_len + 1:
                continue  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ì€ ì œì™¸

            for i in range(len(tokens) - seq_len):
                seq_tokens = tokens[i:i + seq_len]  # ì…ë ¥ ì‹œí€€ìŠ¤
                target_token = tokens[i + seq_len]  # ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡
                seq_indices = [self.word2idx.get(tok, self.word2idx[self.unk_token]) for tok in seq_tokens]
                target_index = self.word2idx.get(target_token, self.word2idx[self.unk_token])
                self.X_data.append(seq_indices)
                self.y_data.append(target_index)

    def __len__(self):
        return len(self.X_data)

    def __getitem__(self, idx):
        return torch.tensor(self.X_data[idx], dtype=torch.long), torch.tensor(self.y_data[idx], dtype=torch.long)

def load_data(data_dir, seq_len=20, batch_size=32):
    """ `data_set/` í´ë” ë‚´ ëª¨ë“  JSON íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ DataLoader ë°˜í™˜ """
    dataset = TextDataset(data_dir, seq_len)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    return loader, dataset
